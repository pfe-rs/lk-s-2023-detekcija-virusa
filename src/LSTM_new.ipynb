{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cIp9xZ9GwpHW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.fasttext import FastText\n",
        "import pickle"
      ],
      "metadata": {
        "id": "I7RSElUobgQb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(MyLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, input_seq):\n",
        "        h0 = torch.zeros(1, input_seq.size(0), self.hidden_size).to(input_seq.device)\n",
        "        c0 = torch.zeros(1, input_seq.size(0), self.hidden_size).to(input_seq.device)\n",
        "        lstm_out, _ = self.lstm(input_seq, (h0, c0))\n",
        "        output = self.fc(lstm_out[:, -1, :])\n",
        "        output = self.softmax(output)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "jyTnmkt0wzHh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading existing model\n",
        "FastTextmodel = FastText.load(\"drive/MyDrive/PFE-Colab/FastText6/fasttext_model\")"
      ],
      "metadata": {
        "id": "lcepkrmJcEQp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the LSTM model\n",
        "input_size = 100  # Assuming each token is a 100-dimensional vector\n",
        "hidden_size = 64\n",
        "output_size = 2  # Number of output classes\n",
        "batch_size = 16\n",
        "num_epochs = 1\n",
        "model = MyLSTM(input_size, hidden_size, output_size)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "6GFvJpYL_Nza"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######### Loading tokens\n",
        "with open(\"drive/MyDrive/tokens_malware.pkl\", \"rb\") as f:\n",
        "    tokenized_log = pickle.load(f)\n",
        "# Generating dummy labels\n",
        "labels = []\n",
        "for i in range(len(tokenized_log)):\n",
        "    labels.append(1)"
      ],
      "metadata": {
        "id": "WrAh5xpzU0GX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batches = []\n",
        "batch_tensors = []\n",
        "for program in tokenized_log:\n",
        "      # Get a list of tokens in a program\n",
        "      sequence_list = []\n",
        "      for line in program:\n",
        "            for token in line:\n",
        "                  vector = FastTextmodel.wv[token]\n",
        "                  sequence_list.append(torch.tensor(vector, dtype=torch.float32))\n",
        "      # Convert list of token tensors to a single sequence tensor\n",
        "      sequence_tensor = torch.stack(sequence_list)\n",
        "      # Add to batch for parallel training on multiple programs at the same time\n",
        "      if len(batch_tensors) == batch_size:\n",
        "            batches.append(batch_tensors)\n",
        "            batch_tensors = []\n",
        "      batch_tensors.append(sequence_tensor)\n",
        "# Last batch may be incomplete due to not enough programs to evenly divide with batch_size\n",
        "if len(batch_tensors) != 0:\n",
        "      batches.append(batch_tensors)\n",
        "      batch_tensors = []\n",
        "\n",
        "\n",
        "# Empty the unused variables\n",
        "tokenized_log = []\n",
        "FastTextmodel = []\n",
        "sequence_list = []\n",
        "sequence_tensor = []\n",
        "batch_tensors = []"
      ],
      "metadata": {
        "id": "c7BGcg1cxBlU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare labels\n",
        "labels_per_batches = [labels[i:i+batch_size] for i in range(0, len(labels), batch_size)]\n",
        "label_tensors = [torch.tensor(batch, dtype=torch.long) for batch in labels_per_batches]\n",
        "\n",
        "labels_per_batches = []"
      ],
      "metadata": {
        "id": "moi0J2L8WBBp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Old custom version - better version below\n",
        "# Train the LSTM model\n",
        "# len(batches) = len(label_tensors)\n",
        "for epoch in range(num_epochs):\n",
        "    for i in range(len(batches)):\n",
        "        batch_tensors = batches[i]\n",
        "        batch_labels = label_tensors[i]\n",
        "        padded_batch = nn.utils.rnn.pad_sequence(batch_tensors, batch_first=True)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(padded_batch)\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "3y3EIFsTQotX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ChatGPT-enhanced version\n",
        "# Train the LSTM model\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0.0  # To accumulate the total loss for the epoch\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    for i in range(len(batches)):\n",
        "        batch_tensors = batches[i]\n",
        "        batch_labels = label_tensors[i]\n",
        "        padded_batch = nn.utils.rnn.pad_sequence(batch_tensors, batch_first=True)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(padded_batch)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(outputs, batch_labels)\n",
        "        epoch_loss += loss.item()  # Accumulate the loss\n",
        "\n",
        "        # Calculate accuracy\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_predictions += (predicted == batch_labels).sum().item()\n",
        "        total_predictions += batch_labels.size(0)\n",
        "\n",
        "        # Backward pass and update weights\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Calculate epoch-level metrics\n",
        "    epoch_loss /= len(batches)  # Calculate average loss for the epoch\n",
        "    accuracy = correct_predictions / total_predictions  # Calculate accuracy\n",
        "\n",
        "    # Print metrics for the epoch\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}] - Loss: {epoch_loss:.4f} - Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "lv4ytpcpYLSd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}